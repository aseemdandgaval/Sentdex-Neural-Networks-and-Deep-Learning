{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQJCnMWvw1Ny"
   },
   "source": [
    "# Sentdex: Neural Networks and Deep Learning 2: Handling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KLZ--SI07tY"
   },
   "source": [
    "* Getting, preparing, formatting etc. data is very important and most of the time is spent on that.\n",
    "* Torchvision is used to import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uocX9hcVwhCM"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_imaging' from 'PIL' (C:\\Users\\aseem\\AppData\\Roaming\\Python\\Python38\\site-packages\\PIL\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dcb4427195e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsun\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSUN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSUNClass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetFolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCocoCaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCIFAR100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstl10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\datasets\\lsun.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"PILLOW_VERSION\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_imaging' from 'PIL' (C:\\Users\\aseem\\AppData\\Roaming\\Python\\Python38\\site-packages\\PIL\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWRRgfKO2ujf"
   },
   "source": [
    "* Its important to split your training and test data sets,     \n",
    "the model should never see the test dataset until the time of testing,                   \n",
    "  otherwise the model will report higher accuracy than it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4r5ui8u3EsF"
   },
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train = True, download = True, \n",
    "                       transform = transforms.Compose([transforms.ToTensor()])) \n",
    "\n",
    "test = datasets.MNIST(\"\", train = False, download = True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVQjU3DeDSOc"
   },
   "source": [
    "* The NN model looks for the shortest path to decrease the loss/cost function.                                                                   \n",
    "We use batches to restrict this in order to train for generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LE2eYaX_37su"
   },
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size =10, shuffle = True)\n",
    "\n",
    "testset = torch.utils.data.DataLoader(test, batch_size =10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEfq7HBXBoc0"
   },
   "outputs": [],
   "source": [
    "for data in trainset:\n",
    "  print(data)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9k_UDJVeBvH_",
    "outputId": "7d0b49a7-1eaa-4577-9eb9-78f5cf4d61a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "nGsRtrcuCJxf",
    "outputId": "db499d64-ffc7-44e8-cb26-f16e0bae4b67"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkFVoyz-CY1n",
    "outputId": "438240fa-4192-411a-c23f-4df7992a0acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjfxBjTWGG5z"
   },
   "source": [
    "* Its important for our data to be good in quaility. Or else tradh in and trash out. \n",
    "\n",
    "* So we need a balanced data set. That means there are the same number of examples for each classification in training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulqxoj2lCegf",
    "outputId": "861bb898-3615-4728-f496-0ad34d1649d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0 }\n",
    "\n",
    "for data in trainset:\n",
    "  Xs, ys = data\n",
    "  for y in ys:\n",
    "    counter_dict[int(y)] += 1\n",
    "    total+=1\n",
    "\n",
    "print(counter_dict)    \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7jZP7qpFHBb",
    "outputId": "1c10e283-0a23-4e04-c9aa-23e59fdfbe3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666%\n",
      "1: 11.236666666666666%\n",
      "2: 9.93%\n",
      "3: 10.218333333333334%\n",
      "4: 9.736666666666666%\n",
      "5: 9.035%\n",
      "6: 9.863333333333333%\n",
      "7: 10.441666666666666%\n",
      "8: 9.751666666666667%\n",
      "9: 9.915000000000001%\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100.0}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC72Bj17HARu"
   },
   "source": [
    "* This data is fairly balanced, no single class has percentage that is too big or too small."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2. Handling_Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
